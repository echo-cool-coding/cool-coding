---
title: Elasticsearch N-gram 分词
description: 了解 Elasticsearch 中的 N-gram 分词器，掌握其工作原理、配置方法以及实际应用场景。
---

# Elasticsearch N-gram 分词

在 Elasticsearch 中，分词（Tokenization）是将文本拆分为单独的词语或标记的过程。N-gram 分词是一种特殊的分词方式，它将文本分割成连续的 N 个字符的片段。这种分词方式在模糊搜索、自动补全和拼写纠错等场景中非常有用。

## 什么是 N-gram 分词？

N-gram 分词是一种将文本按固定长度（N）分割成连续字符片段的技术。例如，对于单词 "elastic"，使用 2-gram（bigram）分词会生成以下片段：`["el", "la", "as", "st", "ti", "ic"]`。

N-gram 分词的优点在于它可以捕捉到文本中的局部模式，从而支持部分匹配和模糊查询。然而，它也会生成大量的分词，可能会增加索引的大小。

## 如何配置 N-gram 分词器？

在 Elasticsearch 中，可以通过自定义分析器来配置 N-gram 分词器。以下是一个配置示例：

```json
PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_ngram_analyzer": {
          "tokenizer": "my_ngram_tokenizer"
        }
      },
      "tokenizer": {
        "my_ngram_tokenizer": {
          "type": "ngram",
          "min_gram": 2,
          "max_gram": 3,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}
```

在这个配置中，我们定义了一个名为 `my_ngram_analyzer` 的分析器，它使用了一个 N-gram 分词器 `my_ngram_tokenizer`。该分词器将文本分割成 2 到 3 个字符的片段，并且只保留字母和数字字符。

## 示例：使用 N-gram 分词器

让我们通过一个示例来理解 N-gram 分词器的工作原理。假设我们有一个文档，内容为 "elasticsearch"，我们希望使用 2-gram 分词器对其进行分词。

```json
POST /my_index/_analyze
{
  "analyzer": "my_ngram_analyzer",
  "text": "elasticsearch"
}
```

执行上述请求后，Elasticsearch 会返回以下分词结果：

```json
{
  "tokens": [
    {
      "token": "el",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "la",
      "start_offset": 1,
      "end_offset": 3,
      "type": "word",
      "position": 1
    },
    {
      "token": "as",
      "start_offset": 2,
      "end_offset": 4,
      "type": "word",
      "position": 2
    },
    // 更多分词...
  ]
}
```

可以看到，文本 "elasticsearch" 被分割成了多个 2-gram 片段。

## 实际应用场景

### 1. 模糊搜索

N-gram 分词器可以用于实现模糊搜索。例如，当用户输入 "elas" 时，系统可以匹配到 "elasticsearch" 中的 "ela" 和 "las" 片段，从而返回相关结果。

### 2. 自动补全

在自动补全功能中，N-gram 分词器可以帮助系统在用户输入部分字符时，快速匹配到可能的完整单词或短语。

### 3. 拼写纠错

N-gram 分词器还可以用于拼写纠错。通过比较用户输入的文本与索引中的 N-gram 片段，系统可以识别出可能的拼写错误并提供纠正建议。

## 总结

N-gram 分词是 Elasticsearch 中一种强大的分词技术，适用于模糊搜索、自动补全和拼写纠错等场景。通过合理配置 N-gram 分词器，可以显著提升搜索体验。

## 附加资源与练习

- **练习**：尝试在 Elasticsearch 中配置一个 3-gram 分词器，并分析一段文本的分词结果。
- **资源**：阅读 Elasticsearch 官方文档中关于 [N-gram 分词器](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html) 的更多内容。

:::tip
在实际应用中，选择合适的 N-gram 长度（`min_gram` 和 `max_gram`）非常重要。过小的 N-gram 可能会生成大量无意义的分词，而过大的 N-gram 可能会降低搜索的灵活性。
:::