---
title: Elasticsearch 分词器
description: 了解Elasticsearch中的分词器，掌握如何通过分词器将文本分解为可搜索的词汇单元，并应用于实际场景。
---

# Elasticsearch 分词器

在Elasticsearch中，**分词器（Analyzer）**是一个非常重要的概念。它负责将文本分解为可搜索的词汇单元（Token），以便在搜索时能够匹配到相关的文档。分词器通常由三个部分组成：**字符过滤器（Character Filters）**、**分词器（Tokenizer）**和**词汇单元过滤器（Token Filters）**。本文将详细介绍这些组件，并通过实际案例展示如何使用它们。

## 什么是分词器？

分词器是Elasticsearch中用于处理文本的工具。它将输入的文本分解为一个个词汇单元（Token），并对这些单元进行标准化处理，以便在搜索时能够匹配到相关的文档。例如，当你搜索“Elasticsearch”时，分词器会将文本分解为“elasticsearch”并将其转换为小写，以便与索引中的文档匹配。

### 分词器的组成

1. **字符过滤器（Character Filters）**：在文本被分词之前，字符过滤器会对文本进行预处理。例如，去除HTML标签、替换特殊字符等。
2. **分词器（Tokenizer）**：将文本分解为词汇单元。例如，将“Hello World”分解为“Hello”和“World”。
3. **词汇单元过滤器（Token Filters）**：对词汇单元进行进一步处理。例如，将词汇单元转换为小写、去除停用词等。

## 内置分词器

Elasticsearch提供了多种内置分词器，以下是几种常用的分词器：

1. **Standard Analyzer**：默认分词器，适用于大多数语言。它将文本分解为词汇单元，并将它们转换为小写。
2. **Whitespace Analyzer**：根据空格将文本分解为词汇单元。
3. **Keyword Analyzer**：将整个文本作为一个词汇单元。
4. **Pattern Analyzer**：使用正则表达式将文本分解为词汇单元。

### 示例：使用Standard Analyzer

```json
POST _analyze
{
  "analyzer": "standard",
  "text": "Hello World!"
}
```

**输出：**

```json
{
  "tokens": [
    {
      "token": "hello",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "world",
      "start_offset": 6,
      "end_offset": 11,
      "type": "<ALPHANUM>",
      "position": 1
    }
  ]
}
```

在这个示例中，`Standard Analyzer`将“Hello World!”分解为“hello”和“world”，并将它们转换为小写。

## 自定义分词器

除了使用内置分词器，你还可以创建自定义分词器。自定义分词器允许你根据需要组合字符过滤器、分词器和词汇单元过滤器。

### 示例：创建自定义分词器

```json
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "char_filter": ["html_strip"],
          "filter": ["lowercase", "stop"]
        }
      }
    }
  }
}
```

在这个示例中，我们创建了一个名为`my_custom_analyzer`的自定义分词器。它使用`standard`分词器，并应用了`html_strip`字符过滤器和`lowercase`、`stop`词汇单元过滤器。

## 实际应用场景

### 场景1：处理HTML内容

假设你有一个包含HTML标签的文本，你希望在索引时去除这些标签。你可以使用`html_strip`字符过滤器来实现这一点。

```json
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": ["html_strip"],
  "text": "<p>Hello <b>World</b>!</p>"
}
```

**输出：**

```json
{
  "tokens": [
    {
      "token": "hello",
      "start_offset": 3,
      "end_offset": 8,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "world",
      "start_offset": 11,
      "end_offset": 16,
      "type": "<ALPHANUM>",
      "position": 1
    }
  ]
}
```

在这个示例中，`html_strip`字符过滤器去除了HTML标签，只保留了文本内容。

### 场景2：去除停用词

假设你希望在索引时去除常见的停用词（如“the”、“and”等）。你可以使用`stop`词汇单元过滤器来实现这一点。

```json
POST _analyze
{
  "tokenizer": "standard",
  "filter": ["lowercase", "stop"],
  "text": "The quick brown fox"
}
```

**输出：**

```json
{
  "tokens": [
    {
      "token": "quick",
      "start_offset": 4,
      "end_offset": 9,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 10,
      "end_offset": 15,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "fox",
      "start_offset": 16,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 3
    }
  ]
}
```

在这个示例中，`stop`词汇单元过滤器去除了“the”这个停用词。

## 总结

Elasticsearch的分词器是处理文本的重要工具。通过理解分词器的组成和工作原理，你可以更好地控制文本的索引和搜索过程。无论是使用内置分词器还是创建自定义分词器，Elasticsearch都提供了强大的功能来满足不同的需求。

:::tip 提示
如果你对分词器的配置有更高的要求，可以尝试使用`Elasticsearch`的`Analysis API`来测试和调试分词器的效果。
:::

## 附加资源

- [Elasticsearch官方文档 - Analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html)
- [Elasticsearch Analysis API](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html)

## 练习

1. 使用`Whitespace Analyzer`对文本“Elasticsearch is powerful”进行分词，并观察输出结果。
2. 创建一个自定义分词器，使用`Pattern Tokenizer`将文本按逗号分隔，并去除停用词。
3. 使用`Analysis API`测试你创建的自定义分词器，并分析输出结果。

通过完成这些练习，你将更深入地理解Elasticsearch分词器的工作原理和应用场景。