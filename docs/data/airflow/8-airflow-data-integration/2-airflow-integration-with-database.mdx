---
title: Airflow 与数据库集成
description: 了解如何使用Apache Airflow与数据库进行集成，管理数据管道和工作流。本文适合初学者，包含代码示例和实际案例。
---

# Airflow 与数据库集成

Apache Airflow 是一个强大的工作流管理工具，广泛用于数据管道的编排和调度。与数据库的集成是 Airflow 的核心功能之一，它允许用户从数据库中提取数据、将数据写入数据库，并在工作流中执行复杂的数据库操作。本文将逐步介绍如何在 Airflow 中与数据库进行集成，并提供实际案例和代码示例。

## 1. 什么是 Airflow 与数据库集成？

Airflow 与数据库集成是指通过 Airflow 的任务（Task）与数据库进行交互，执行诸如查询、插入、更新和删除等操作。这种集成使得 Airflow 能够自动化数据管道的各个阶段，从数据提取到数据加载（ETL），再到数据转换和存储。

Airflow 提供了多种方式来与数据库进行交互，包括使用内置的数据库操作符（Operator）或通过自定义 Python 脚本来执行 SQL 查询。

## 2. 准备工作

在开始之前，确保你已经安装了 Airflow 并配置了数据库连接。Airflow 支持多种数据库，包括 PostgreSQL、MySQL、SQLite 和 Oracle 等。你可以通过 Airflow 的 Web UI 或配置文件来设置数据库连接。

### 2.1 配置数据库连接

在 Airflow 中，数据库连接是通过 **Connection** 对象来管理的。你可以在 Airflow 的 Web UI 中导航到 **Admin > Connections** 来添加或编辑数据库连接。

例如，添加一个 PostgreSQL 数据库连接：

- **Conn Id**: `my_postgres_conn`
- **Conn Type**: `Postgres`
- **Host**: `localhost`
- **Schema**: `mydatabase`
- **Login**: `myuser`
- **Password**: `mypassword`
- **Port**: `5432`

## 3. 使用 Airflow 操作符与数据库交互

Airflow 提供了多种内置的操作符来与数据库进行交互。以下是一些常用的操作符：

- `PostgresOperator`: 用于执行 PostgreSQL 查询。
- `MySqlOperator`: 用于执行 MySQL 查询。
- `SqliteOperator`: 用于执行 SQLite 查询。
- `OracleOperator`: 用于执行 Oracle 查询。

### 3.1 使用 PostgresOperator 执行查询

以下是一个使用 `PostgresOperator` 执行 SQL 查询的示例：

```python
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

with DAG('postgres_example', default_args=default_args, schedule_interval='@daily') as dag:
    create_table = PostgresOperator(
        task_id='create_table',
        postgres_conn_id='my_postgres_conn',
        sql="""
        CREATE TABLE IF NOT EXISTS my_table (
            id SERIAL PRIMARY KEY,
            name VARCHAR(255) NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
    )

    insert_data = PostgresOperator(
        task_id='insert_data',
        postgres_conn_id='my_postgres_conn',
        sql="""
        INSERT INTO my_table (name) VALUES ('Alice'), ('Bob'), ('Charlie');
        """
    )

    create_table >> insert_data
```

在这个示例中，我们定义了一个 DAG，其中包含两个任务：`create_table` 和 `insert_data`。`create_table` 任务用于创建一个名为 `my_table` 的表，而 `insert_data` 任务则向该表中插入一些数据。

### 3.2 使用 PythonOperator 执行自定义 SQL

如果你需要执行更复杂的数据库操作，可以使用 `PythonOperator` 来编写自定义的 Python 脚本。以下是一个使用 `psycopg2` 库与 PostgreSQL 数据库交互的示例：

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
import psycopg2

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

def query_database():
    conn = psycopg2.connect(
        dbname="mydatabase",
        user="myuser",
        password="mypassword",
        host="localhost",
        port="5432"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM my_table;")
    results = cursor.fetchall()
    for row in results:
        print(row)
    cursor.close()
    conn.close()

with DAG('custom_sql_example', default_args=default_args, schedule_interval='@daily') as dag:
    query_task = PythonOperator(
        task_id='query_database',
        python_callable=query_database
    )

    query_task
```

在这个示例中，我们使用 `PythonOperator` 来执行一个自定义的 Python 函数 `query_database`，该函数连接到 PostgreSQL 数据库并查询 `my_table` 表中的所有数据。

## 4. 实际案例：自动化数据管道

假设你有一个数据管道，需要每天从多个数据源提取数据，将其加载到 PostgreSQL 数据库中，并进行一些数据转换操作。以下是一个简化的示例，展示了如何使用 Airflow 自动化这个过程：

```python
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
import pandas as pd
import psycopg2

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

def extract_transform_load():
    # 提取数据
    data = pd.read_csv('data_source.csv')
    
    # 数据转换
    data['new_column'] = data['existing_column'] * 2
    
    # 加载数据到数据库
    conn = psycopg2.connect(
        dbname="mydatabase",
        user="myuser",
        password="mypassword",
        host="localhost",
        port="5432"
    )
    cursor = conn.cursor()
    for index, row in data.iterrows():
        cursor.execute(
            "INSERT INTO my_table (name, new_column) VALUES (%s, %s);",
            (row['name'], row['new_column'])
        )
    conn.commit()
    cursor.close()
    conn.close()

with DAG('etl_pipeline', default_args=default_args, schedule_interval='@daily') as dag:
    create_table = PostgresOperator(
        task_id='create_table',
        postgres_conn_id='my_postgres_conn',
        sql="""
        CREATE TABLE IF NOT EXISTS my_table (
            id SERIAL PRIMARY KEY,
            name VARCHAR(255) NOT NULL,
            new_column FLOAT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
    )

    etl_task = PythonOperator(
        task_id='etl_task',
        python_callable=extract_transform_load
    )

    create_table >> etl_task
```

在这个案例中，我们定义了一个 ETL 管道，每天从 CSV 文件中提取数据，进行简单的数据转换，然后将结果加载到 PostgreSQL 数据库中。

## 5. 总结

通过本文，你已经了解了如何在 Airflow 中与数据库进行集成。我们介绍了如何使用内置的操作符执行 SQL 查询，以及如何使用自定义 Python 脚本进行更复杂的数据库操作。我们还通过一个实际案例展示了如何自动化数据管道。

## 6. 附加资源与练习

- **练习**: 尝试使用 `MySqlOperator` 或 `SqliteOperator` 创建一个类似的 DAG，执行数据库操作。
- **资源**: 阅读 [Airflow 官方文档](https://airflow.apache.org/docs/) 以了解更多关于数据库集成的详细信息。

:::tip
如果你在配置数据库连接时遇到问题，可以查看 Airflow 的日志文件以获取更多调试信息。
:::