---
title: Airflow Docker部署
description: "学习如何使用 Docker 部署 Apache Airflow，适合初学者的全面指南。"
---

# Airflow Docker部署

Apache Airflow 是一个强大的工作流管理工具，广泛用于数据管道的编排和调度。为了简化部署过程，Docker 成为了一个流行的选择。本文将逐步介绍如何使用 Docker 部署 Airflow，并提供实际案例和代码示例。

## 什么是 Docker？

Docker 是一个开源平台，用于自动化应用程序的部署、扩展和管理。它通过容器化技术，将应用程序及其依赖项打包在一起，确保在任何环境中都能一致运行。

## 为什么使用 Docker 部署 Airflow？

使用 Docker 部署 Airflow 有以下几个优点：

- **环境一致性**：Docker 容器确保开发、测试和生产环境的一致性。
- **简化部署**：Docker 简化了 Airflow 的安装和配置过程。
- **可扩展性**：Docker 使得扩展 Airflow 集群变得更加容易。

## 准备工作

在开始之前，请确保你已经安装了以下工具：

- Docker
- Docker Compose

## 部署步骤

### 1. 创建 `docker-compose.yml` 文件

首先，创建一个名为 `docker-compose.yml` 的文件，用于定义 Airflow 服务的配置。

```yaml
version: '3.8'
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  webserver:
    image: apache/airflow:2.2.3
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    command: webserver
    depends_on:
      - postgres

  scheduler:
    image: apache/airflow:2.2.3
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
    command: scheduler
    depends_on:
      - postgres

volumes:
  postgres-db-volume:
```

### 2. 启动 Airflow 服务

在终端中运行以下命令来启动 Airflow 服务：

```bash
docker-compose up -d
```

### 3. 访问 Airflow Web UI

启动服务后，你可以通过浏览器访问 Airflow 的 Web UI，地址为 `http://localhost:8080`。默认的用户名和密码都是 `airflow`。

## 实际案例

假设你有一个简单的 DAG 文件 `example_dag.py`，用于每天打印 "Hello, World!"。

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def print_hello():
    print("Hello, World!")

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

dag = DAG(
    'example_dag',
    default_args=default_args,
    schedule_interval='@daily',
)

task = PythonOperator(
    task_id='print_hello',
    python_callable=print_hello,
    dag=dag,
)
```

将 `example_dag.py` 文件放入 `dags` 目录中，Airflow 将自动加载并调度该 DAG。

## 总结

通过 Docker 部署 Airflow，你可以轻松地管理和扩展你的工作流。本文介绍了如何使用 Docker Compose 部署 Airflow，并提供了一个简单的 DAG 示例。希望这篇指南能帮助你快速上手 Airflow 的 Docker 部署。

## 附加资源

- [Apache Airflow 官方文档](https://airflow.apache.org/docs/)
- [Docker 官方文档](https://docs.docker.com/)
- [Docker Compose 官方文档](https://docs.docker.com/compose/)

## 练习

1. 尝试修改 `docker-compose.yml` 文件，使用 `CeleryExecutor` 替代 `LocalExecutor`。
2. 创建一个新的 DAG，用于每天从 API 获取数据并存储到数据库中。

:::tip
如果你在部署过程中遇到问题，可以查看 Docker 容器的日志来获取更多信息：

```bash
docker-compose logs -f
```
:::