---
title: Airflow 单机部署
description: 本指南将详细介绍如何在单机环境下部署Apache Airflow，适合初学者快速上手。
---

# Airflow 单机部署

Apache Airflow 是一个开源的工作流管理平台，用于编排和调度复杂的数据管道。它通过有向无环图（DAG）来定义任务依赖关系，并提供了丰富的调度和监控功能。对于初学者来说，单机部署是学习和理解 Airflow 的最佳起点。

## 什么是单机部署？

单机部署是指在一台机器上运行 Airflow 的所有组件，包括 Web 服务器、调度器、执行器和数据库。这种部署方式适合开发、测试和小规模生产环境。

## 准备工作

在开始部署之前，请确保你的机器满足以下要求：

- Python 3.6 或更高版本
- pip 包管理工具
- 一个可用的数据库（如 SQLite、PostgreSQL 或 MySQL）

:::tip
对于初学者，建议使用 SQLite 作为数据库，因为它无需额外配置即可使用。
:::

## 安装 Airflow

首先，使用 pip 安装 Airflow：

```bash
pip install apache-airflow
```

安装完成后，初始化 Airflow 数据库：

```bash
airflow db init
```

此命令会创建一个 SQLite 数据库文件 `airflow.db`，并生成默认的配置文件 `airflow.cfg`。

## 配置 Airflow

默认情况下，Airflow 使用 SQLite 作为数据库，并且 Web 服务器和调度器都在本地运行。你可以通过编辑 `airflow.cfg` 文件来修改配置。

例如，如果你想使用 PostgreSQL 作为数据库，可以修改以下配置：

```ini
[core]
sql_alchemy_conn = postgresql+psycopg2://user:password@localhost:5432/airflow
```

:::caution
确保在修改配置文件后重新初始化数据库：

```bash
airflow db init
```
:::

## 启动 Airflow 组件

### 启动 Web 服务器

Web 服务器提供了一个用户界面，用于监控和管理 DAG。启动 Web 服务器：

```bash
airflow webserver --port 8080
```

访问 `http://localhost:8080` 即可看到 Airflow 的 Web 界面。

### 启动调度器

调度器负责解析 DAG 并调度任务。启动调度器：

```bash
airflow scheduler
```

## 创建并运行第一个 DAG

在 `AIRFLOW_HOME/dags` 目录下创建一个 Python 文件 `example_dag.py`，内容如下：

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

dag = DAG(
    'example_dag',
    default_args=default_args,
    description='A simple tutorial DAG',
    schedule_interval='@daily',
)

t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

t2 = BashOperator(
    task_id='sleep',
    bash_command='sleep 5',
    retries=3,
    dag=dag,
)

t1 >> t2
```

保存文件后，Airflow 会自动加载该 DAG。你可以在 Web 界面中看到并触发它。

## 实际应用场景

假设你有一个每天需要运行的数据处理任务，例如从数据库中提取数据、进行转换并加载到另一个系统中。你可以使用 Airflow 来编排这些任务，并确保它们在正确的时间按顺序执行。

## 总结

通过本指南，你已经成功在单机环境下部署了 Apache Airflow，并创建并运行了第一个 DAG。单机部署是学习和测试 Airflow 的理想方式，但在生产环境中，你可能需要考虑分布式部署以提高性能和可靠性。

## 附加资源

- [Apache Airflow 官方文档](https://airflow.apache.org/docs/)
- [Airflow GitHub 仓库](https://github.com/apache/airflow)
- [Airflow 社区论坛](https://airflow.apache.org/community/)

## 练习

1. 修改 `example_dag.py`，添加一个新的任务，该任务使用 `PythonOperator` 来打印 "Hello, Airflow!"。
2. 尝试将数据库从 SQLite 切换到 PostgreSQL，并观察 Airflow 的行为变化。
