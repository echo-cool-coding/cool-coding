---
title: Airflow 与Azure集成
description: 了解如何将Apache Airflow与Microsoft Azure集成，以管理和编排云工作流。本文适合初学者，包含详细的步骤、代码示例和实际应用场景。
---

## 介绍

Apache Airflow 是一个开源的工作流编排工具，用于调度和监控复杂的数据管道。Microsoft Azure 是一个广泛使用的云平台，提供各种服务，如虚拟机、存储、数据库和机器学习工具。将 Airflow 与 Azure 集成，可以帮助您自动化和管理在 Azure 上运行的云工作流。

本文将逐步介绍如何将 Airflow 与 Azure 集成，包括设置 Azure 服务、配置 Airflow 连接以及编写 DAG（有向无环图）来管理 Azure 任务。

## 前提条件

在开始之前，请确保您具备以下条件：

1. 一个 Azure 账户，并已创建资源组。
2. 已安装并配置好 Apache Airflow。
3. 基本的 Python 编程知识。

## 步骤 1：设置 Azure 服务

首先，您需要在 Azure 上创建一些服务，以便 Airflow 可以与之交互。以下是一些常见的 Azure 服务，您可能需要与 Airflow 集成：

- **Azure Blob Storage**：用于存储数据。
- **Azure Data Factory**：用于数据集成和 ETL 操作。
- **Azure Kubernetes Service (AKS)**：用于容器化应用程序的部署和管理。

### 创建 Azure Blob Storage

1. 登录 Azure 门户。
2. 导航到“存储账户”并点击“创建”。
3. 填写所需信息，如订阅、资源组和存储账户名称。
4. 点击“查看 + 创建”，然后点击“创建”。

创建完成后，记下存储账户名称和访问密钥，稍后将在 Airflow 中配置连接时使用。

## 步骤 2：配置 Airflow 连接

在 Airflow 中，您需要配置与 Azure 服务的连接。这可以通过 Airflow 的 Web UI 或直接通过代码完成。

### 通过 Web UI 配置连接

1. 打开 Airflow Web UI。
2. 导航到“Admin” > “Connections”。
3. 点击“Create”按钮。
4. 在“Conn Id”字段中输入 `azure_blob_storage`。
5. 在“Conn Type”字段中选择 `Azure Blob Storage`。
6. 在“Extra”字段中输入以下 JSON 配置：

```json
{
  "account_name": "<your_storage_account_name>",
  "account_key": "<your_storage_account_key>"
}
```

7. 点击“Save”保存连接。

### 通过代码配置连接

您也可以在 Airflow DAG 中通过代码配置连接：

```python
from airflow import DAG
from airflow.providers.microsoft.azure.hooks.azure_blob_storage import AzureBlobStorageHook
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def upload_to_blob_storage():
    hook = AzureBlobStorageHook(conn_id='azure_blob_storage')
    hook.upload_file('my_container', 'my_blob', 'path/to/local/file')

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
}

with DAG('azure_blob_storage_example', default_args=default_args, schedule_interval='@daily') as dag:
    upload_task = PythonOperator(
        task_id='upload_to_blob_storage',
        python_callable=upload_to_blob_storage
    )

    upload_task
```

## 步骤 3：编写 DAG 以管理 Azure 任务

接下来，您可以编写一个 DAG 来管理 Azure 上的任务。以下是一个简单的 DAG 示例，它将文件上传到 Azure Blob Storage：

```python
from airflow import DAG
from airflow.providers.microsoft.azure.hooks.azure_blob_storage import AzureBlobStorageHook
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def upload_to_blob_storage():
    hook = AzureBlobStorageHook(conn_id='azure_blob_storage')
    hook.upload_file('my_container', 'my_blob', 'path/to/local/file')

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
}

with DAG('azure_blob_storage_example', default_args=default_args, schedule_interval='@daily') as dag:
    upload_task = PythonOperator(
        task_id='upload_to_blob_storage',
        python_callable=upload_to_blob_storage
    )

    upload_task
```

在这个示例中，我们定义了一个 Python 函数 `upload_to_blob_storage`，它使用 `AzureBlobStorageHook` 将本地文件上传到 Azure Blob Storage。然后，我们创建了一个 DAG，每天执行一次上传任务。

## 实际应用场景

假设您有一个每天生成的数据文件，需要上传到 Azure Blob Storage 以供进一步处理。您可以使用上述 DAG 来自动化这个过程。此外，您还可以扩展这个 DAG，以在文件上传后触发 Azure Data Factory 的 ETL 管道，或者将文件内容加载到 Azure SQL 数据库中。

## 总结

通过将 Apache Airflow 与 Microsoft Azure 集成，您可以轻松管理和自动化云工作流。本文介绍了如何设置 Azure 服务、配置 Airflow 连接以及编写 DAG 来管理 Azure 任务。希望这些内容能帮助您更好地理解和使用 Airflow 与 Azure 的集成。

## 附加资源

- [Apache Airflow 官方文档](https://airflow.apache.org/docs/)
- [Microsoft Azure 官方文档](https://docs.microsoft.com/en-us/azure/)
- [Airflow Azure Provider 文档](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/index.html)

## 练习

1. 创建一个新的 Azure Blob Storage 容器，并编写一个 DAG 将多个文件上传到该容器。
2. 扩展上述 DAG，使其在上传文件后触发 Azure Data Factory 的 ETL 管道。
3. 尝试将 Airflow 与 Azure Kubernetes Service (AKS) 集成，以管理容器化应用程序的部署。

:::tip
在完成这些练习时，请确保您已经正确配置了 Azure 服务和 Airflow 连接，并仔细检查 DAG 的日志以排查任何潜在问题。
:::