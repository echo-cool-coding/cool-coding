---
title: Airflow 安装与配置
description: 本教程将详细介绍如何安装和配置Apache Airflow，帮助初学者快速上手这一强大的工作流管理工具。
---

# Airflow 安装与配置

Apache Airflow 是一个开源的工作流管理平台，用于编排复杂的数据管道。它允许用户以编程方式定义、调度和监控工作流。本文将逐步指导您如何安装和配置 Airflow，以便您能够开始使用它来管理您的工作流。

## 1. 安装 Airflow

### 1.1 环境准备

在安装 Airflow 之前，您需要确保您的系统满足以下要求：

- Python 3.6 或更高版本
- pip（Python 包管理工具）

您可以通过以下命令检查 Python 和 pip 的版本：

```bash
python --version
pip --version
```

### 1.2 安装 Airflow

Airflow 可以通过 pip 安装。建议在虚拟环境中安装 Airflow，以避免与其他 Python 项目产生冲突。

首先，创建一个虚拟环境并激活它：

```bash
python -m venv airflow_env
source airflow_env/bin/activate
```

接下来，使用 pip 安装 Airflow：

```bash
pip install apache-airflow
```

:::note
默认情况下，Airflow 会安装所有核心组件。如果您只需要特定的组件，可以使用 `pip install apache-airflow[<component>]` 来安装。例如，`pip install apache-airflow[postgres]` 会安装 Airflow 和 PostgreSQL 相关的依赖。
:::

### 1.3 初始化数据库

Airflow 使用数据库来存储元数据。在首次使用 Airflow 之前，您需要初始化数据库：

```bash
airflow db init
```

此命令将创建一个 SQLite 数据库（默认情况下），并生成必要的表结构。

## 2. 配置 Airflow

### 2.1 配置数据库

默认情况下，Airflow 使用 SQLite 作为数据库。对于生产环境，建议使用更强大的数据库，如 PostgreSQL 或 MySQL。

要更改数据库配置，请编辑 `airflow.cfg` 文件中的 `sql_alchemy_conn` 配置项。例如，使用 PostgreSQL 数据库：

```ini
sql_alchemy_conn = postgresql+psycopg2://user:password@localhost/airflow
```

然后，重新初始化数据库：

```bash
airflow db init
```

### 2.2 配置执行器

Airflow 支持多种执行器，如 `SequentialExecutor`、`LocalExecutor` 和 `CeleryExecutor`。默认情况下，Airflow 使用 `SequentialExecutor`，它只能顺序执行任务。

要更改执行器，请编辑 `airflow.cfg` 文件中的 `executor` 配置项。例如，使用 `LocalExecutor`：

```ini
executor = LocalExecutor
```

### 2.3 配置 Web 服务器

Airflow 提供了一个 Web 界面，用于监控和管理工作流。要启动 Web 服务器，请运行以下命令：

```bash
airflow webserver
```

默认情况下，Web 服务器会在 `http://localhost:8080` 上运行。您可以通过编辑 `airflow.cfg` 文件中的 `web_server_port` 配置项来更改端口。

### 2.4 配置调度器

调度器是 Airflow 的核心组件，负责调度和执行任务。要启动调度器，请运行以下命令：

```bash
airflow scheduler
```

## 3. 实际案例

### 3.1 创建第一个 DAG

DAG（有向无环图）是 Airflow 中工作流的基本单位。以下是一个简单的 DAG 示例，它每天打印 "Hello, World!"：

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

dag = DAG(
    'hello_world',
    default_args=default_args,
    description='A simple tutorial DAG',
    schedule_interval='@daily',
)

t1 = BashOperator(
    task_id='print_hello',
    bash_command='echo "Hello, World!"',
    dag=dag,
)
```

将此代码保存为 `hello_world.py`，并放置在 Airflow 的 `dags` 目录中。Airflow 会自动加载并调度此 DAG。

### 3.2 监控 DAG

启动 Web 服务器后，您可以在 `http://localhost:8080` 上查看 DAG 的状态。您还可以手动触发 DAG 的执行，并查看任务的日志。

## 4. 总结

通过本教程，您已经成功安装并配置了 Apache Airflow，并创建了第一个简单的 DAG。Airflow 是一个功能强大的工具，适用于各种复杂的工作流管理场景。接下来，您可以探索更多高级功能，如使用不同的执行器、配置电子邮件通知、以及集成其他数据源。

## 5. 附加资源

- [Airflow 官方文档](https://airflow.apache.org/docs/)
- [Airflow GitHub 仓库](https://github.com/apache/airflow)
- [Airflow 社区论坛](https://airflow.apache.org/community/)

## 6. 练习

1. 修改 `hello_world.py`，使其每天打印不同的消息。
2. 尝试使用 `LocalExecutor` 并观察任务并行执行的效果。
3. 配置 Airflow 使用 PostgreSQL 数据库，并验证其功能。

祝您学习愉快！