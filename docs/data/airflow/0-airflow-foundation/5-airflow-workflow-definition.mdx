---
title: Airflow 工作流定义
description: "了解如何在 Apache Airflow 中定义工作流，包括 DAG 的基本结构、任务定义以及依赖关系的设置。"
---

## 介绍

Apache Airflow 是一个用于编排复杂工作流的开源工具。它通过定义有向无环图（DAG）来管理任务的执行顺序和依赖关系。本文将详细介绍如何在 Airflow 中定义工作流，包括 DAG 的基本结构、任务定义以及任务之间的依赖关系。

## DAG 的基本结构

在 Airflow 中，工作流是通过 DAG（Directed Acyclic Graph）来定义的。DAG 是一个由任务组成的有向无环图，每个任务代表一个操作或步骤。DAG 定义了任务的执行顺序和依赖关系。

### 创建 DAG

首先，我们需要导入必要的模块并创建一个 DAG 对象。以下是一个简单的 DAG 定义示例：

```python
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime

# 定义 DAG
dag = DAG(
    'my_first_dag',
    description='A simple tutorial DAG',
    schedule_interval='@daily',
    start_date=datetime(2023, 1, 1),
    catchup=False
)
```

在这个示例中，我们定义了一个名为 `my_first_dag` 的 DAG，它每天执行一次，从 2023 年 1 月 1 日开始。

### 定义任务

在 DAG 中，任务是通过操作符（Operator）来定义的。Airflow 提供了多种操作符，例如 `DummyOperator`、`PythonOperator`、`BashOperator` 等。以下是一个使用 `DummyOperator` 定义任务的示例：

```python
# 定义任务
start_task = DummyOperator(task_id='start_task', dag=dag)
end_task = DummyOperator(task_id='end_task', dag=dag)
```

在这个示例中，我们定义了两个任务：`start_task` 和 `end_task`。

### 设置任务依赖关系

任务之间的依赖关系可以通过 `>>` 或 `<<` 操作符来设置。以下是一个设置任务依赖关系的示例：

```python
# 设置任务依赖关系
start_task >> end_task
```

在这个示例中，`start_task` 必须在 `end_task` 之前执行。

## 实际案例

假设我们需要定义一个工作流，该工作流包含以下步骤：

1. 下载数据
2. 处理数据
3. 保存处理后的数据

我们可以使用 `BashOperator` 和 `PythonOperator` 来定义这些任务。以下是一个完整的示例：

```python
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator

def process_data():
    print("Processing data...")

# 定义任务
download_data = BashOperator(
    task_id='download_data',
    bash_command='echo "Downloading data..."',
    dag=dag
)

process_data = PythonOperator(
    task_id='process_data',
    python_callable=process_data,
    dag=dag
)

save_data = BashOperator(
    task_id='save_data',
    bash_command='echo "Saving data..."',
    dag=dag
)

# 设置任务依赖关系
download_data >> process_data >> save_data
```

在这个示例中，我们定义了三个任务：`download_data`、`process_data` 和 `save_data`，并设置了它们之间的依赖关系。

## 总结

在本文中，我们介绍了如何在 Apache Airflow 中定义工作流。我们首先创建了一个 DAG，然后定义了任务并设置了任务之间的依赖关系。最后，我们通过一个实际案例展示了如何将这些概念应用到实际场景中。

## 附加资源

- [Apache Airflow 官方文档](https://airflow.apache.org/docs/)
- [Airflow 教程](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- [Airflow 操作符参考](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html)

## 练习

1. 创建一个包含三个任务的 DAG，任务分别为 `task1`、`task2` 和 `task3`，并设置 `task1` 必须在 `task2` 之前执行，`task2` 必须在 `task3` 之前执行。
2. 使用 `PythonOperator` 定义一个任务，该任务调用一个 Python 函数，函数的功能是打印当前日期。
3. 修改上述实际案例中的 `process_data` 任务，使其在处理数据后生成一个日志文件。

通过完成这些练习，您将更好地理解如何在 Airflow 中定义和管理工作流。