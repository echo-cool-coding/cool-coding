---
title: Hadoop 安装与配置
description: 本文详细介绍了如何在本地环境中安装和配置Hadoop，适合初学者快速上手。
---

## 介绍

Hadoop是一个开源的分布式计算框架，广泛用于处理大规模数据集。它由HDFS（Hadoop分布式文件系统）和MapReduce计算模型组成。为了开始使用Hadoop，首先需要在本地或集群环境中安装和配置它。本文将逐步指导你完成Hadoop的安装与配置过程。

## 环境准备

在开始安装Hadoop之前，确保你的系统满足以下要求：

- **操作系统**：Linux（推荐Ubuntu或CentOS）或 macOS。
- **Java**：Hadoop需要Java 8或更高版本。可以通过以下命令检查Java版本：

  ```bash
  java -version
  ```

  如果未安装Java，可以使用以下命令安装：

  ```bash
  sudo apt-get install openjdk-8-jdk
  ```

- **SSH**：Hadoop使用SSH进行节点间的通信。确保SSH已安装并配置无密码登录：

  ```bash
  sudo apt-get install ssh
  ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  chmod 0600 ~/.ssh/authorized_keys
  ```

## 下载与安装Hadoop

1. **下载Hadoop**：访问[Hadoop官网](https://hadoop.apache.org/releases.html)下载最新稳定版本的Hadoop。本文以Hadoop 3.3.1为例。

   ```bash
   wget https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
   ```

2. **解压Hadoop**：将下载的压缩包解压到指定目录。

   ```bash
   tar -xzvf hadoop-3.3.1.tar.gz -C /usr/local/
   ```

3. **设置环境变量**：编辑`~/.bashrc`文件，添加Hadoop的环境变量。

   ```bash
   export HADOOP_HOME=/usr/local/hadoop-3.3.1
   export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   ```

   然后使环境变量生效：

   ```bash
   source ~/.bashrc
   ```

## 配置Hadoop

Hadoop的配置文件位于`$HADOOP_HOME/etc/hadoop/`目录下。以下是几个关键配置文件的修改：

1. **hadoop-env.sh**：设置Java环境变量。

   ```bash
   export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
   ```

2. **core-site.xml**：配置HDFS的默认文件系统。

   ```xml
   <configuration>
       <property>
           <name>fs.defaultFS</name>
           <value>hdfs://localhost:9000</value>
       </property>
   </configuration>
   ```

3. **hdfs-site.xml**：配置HDFS的副本数和数据存储目录。

   ```xml
   <configuration>
       <property>
           <name>dfs.replication</name>
           <value>1</value>
       </property>
       <property>
           <name>dfs.namenode.name.dir</name>
           <value>/usr/local/hadoop-3.3.1/data/namenode</value>
       </property>
       <property>
           <name>dfs.datanode.data.dir</name>
           <value>/usr/local/hadoop-3.3.1/data/datanode</value>
       </property>
   </configuration>
   ```

4. **mapred-site.xml**：配置MapReduce框架。

   ```xml
   <configuration>
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ```

5. **yarn-site.xml**：配置YARN资源管理器。

   ```xml
   <configuration>
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
   </configuration>
   ```

## 启动Hadoop

1. **格式化HDFS**：在首次启动Hadoop之前，需要格式化HDFS。

   ```bash
   hdfs namenode -format
   ```

2. **启动HDFS**：启动HDFS服务。

   ```bash
   start-dfs.sh
   ```

3. **启动YARN**：启动YARN资源管理器。

   ```bash
   start-yarn.sh
   ```

4. **验证启动**：使用`jps`命令查看Hadoop相关进程是否启动。

   ```bash
   jps
   ```

   输出应包含`NameNode`、`DataNode`、`ResourceManager`和`NodeManager`等进程。

## 实际案例

假设你有一个文本文件`input.txt`，内容如下：

```
Hello World
Hello Hadoop
```

你可以使用Hadoop的MapReduce程序来统计每个单词的出现次数。

1. **上传文件到HDFS**：

   ```bash
   hdfs dfs -mkdir /input
   hdfs dfs -put input.txt /input
   ```

2. **运行WordCount程序**：

   ```bash
   hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input /output
   ```

3. **查看结果**：

   ```bash
   hdfs dfs -cat /output/part-r-00000
   ```

   输出应为：

   ```
   Hadoop   1
   Hello    2
   World    1
   ```

## 总结

通过本文，你已经成功安装并配置了Hadoop，并运行了一个简单的MapReduce程序。Hadoop的强大之处在于其分布式处理能力，能够处理PB级别的数据。接下来，你可以尝试在集群环境中部署Hadoop，以充分利用其分布式计算能力。

## 附加资源

- [Hadoop官方文档](https://hadoop.apache.org/docs/stable/)
- [Hadoop入门教程](https://www.tutorialspoint.com/hadoop/index.htm)
- [Hadoop实战书籍推荐](https://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/1491901632)

:::tip
**练习**：尝试在Hadoop集群中运行一个更复杂的MapReduce程序，例如排序或连接操作。
:::