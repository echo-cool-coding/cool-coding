---
title: PyTorch 梯度下降
description: 了解如何使用PyTorch实现梯度下降，优化机器学习模型中的参数。
---

# PyTorch 梯度下降

梯度下降（Gradient Descent）是机器学习中最常用的优化算法之一。它的核心思想是通过迭代调整模型的参数，以最小化损失函数。PyTorch 提供了强大的自动微分功能，使得实现梯度下降变得非常简单。本文将带你从基础概念入手，逐步实现梯度下降，并通过实际案例展示其应用。

## 什么是梯度下降？

梯度下降是一种优化算法，用于找到函数的最小值。在机器学习中，我们通常希望最小化损失函数，以找到最佳的模型参数。梯度下降通过计算损失函数关于模型参数的梯度（即导数），并沿着梯度的反方向更新参数，从而逐步接近最小值。

### 梯度下降的数学原理

假设我们有一个损失函数 $ L(\theta) $，其中 $ \theta $ 是模型的参数。梯度下降的更新规则如下：

$$
\theta = \theta - \eta \cdot \nabla_\theta L(\theta)
$$

其中：
- $ \eta $ 是学习率（learning rate），控制每次更新的步长。
- $ \nabla_\theta L(\theta) $ 是损失函数关于参数 $ \theta $ 的梯度。

## PyTorch 中的梯度下降

PyTorch 提供了自动微分功能，可以自动计算梯度。我们可以利用这一功能轻松实现梯度下降。

### 1. 定义模型和损失函数

首先，我们需要定义一个简单的模型和损失函数。假设我们有一个线性模型：

```python
import torch

# 定义模型参数
theta = torch.tensor([1.0, 2.0], requires_grad=True)

# 定义损失函数
def loss_fn(theta):
    return (theta[0] - 3) ** 2 + (theta[1] - 5) ** 2
```

### 2. 计算梯度并更新参数

接下来，我们可以使用 PyTorch 的自动微分功能来计算梯度，并更新参数：

```python
# 设置学习率
learning_rate = 0.1

# 迭代更新参数
for i in range(100):
    # 计算损失
    loss = loss_fn(theta)
    
    # 反向传播计算梯度
    loss.backward()
    
    # 更新参数
    with torch.no_grad():
        theta -= learning_rate * theta.grad
    
    # 清零梯度
    theta.grad.zero_()

    # 打印损失
    if i % 10 == 0:
        print(f"Iteration {i}: Loss = {loss.item()}, Theta = {theta}")
```

### 3. 输出结果

运行上述代码后，你会看到损失函数的值逐渐减小，参数 $ \theta $ 也逐渐接近最优值。

```
Iteration 0: Loss = 20.0, Theta = tensor([1.2000, 2.4000], requires_grad=True)
Iteration 10: Loss = 0.8192, Theta = tensor([2.3600, 4.1200], requires_grad=True)
Iteration 20: Loss = 0.0336, Theta = tensor([2.8720, 4.7440], requires_grad=True)
...
Iteration 90: Loss = 0.0000, Theta = tensor([3.0000, 5.0000], requires_grad=True)
```

## 实际应用案例

梯度下降在机器学习中有广泛的应用。例如，在训练神经网络时，我们通常使用梯度下降来优化网络的权重和偏置。以下是一个简单的线性回归模型的例子：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 生成一些随机数据
x = torch.randn(100, 1)
y = 3 * x + 5 + 0.1 * torch.randn(100, 1)

# 定义线性模型
model = nn.Linear(1, 1)

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 训练模型
for epoch in range(100):
    # 前向传播
    y_pred = model(x)
    
    # 计算损失
    loss = criterion(y_pred, y)
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 打印损失
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: Loss = {loss.item()}")
```

在这个例子中，我们使用梯度下降来优化线性回归模型的参数，使其能够更好地拟合数据。

## 总结

梯度下降是机器学习中最重要的优化算法之一。通过 PyTorch 的自动微分功能，我们可以轻松实现梯度下降，并应用于各种机器学习任务中。本文介绍了梯度下降的基本原理，并通过代码示例展示了如何在 PyTorch 中实现梯度下降。

## 附加资源与练习

- **练习 1**：尝试调整学习率，观察对训练过程的影响。
- **练习 2**：使用梯度下降优化一个简单的二次函数，例如 $ f(x) = x^2 $。
- **附加资源**：
  - [PyTorch 官方文档](https://pytorch.org/docs/stable/index.html)
  - 《深度学习》 by Ian Goodfellow, Yoshua Bengio, and Aaron Courville

:::tip
如果你对梯度下降的原理还有疑问，建议从数学角度深入理解梯度的概念，这将帮助你更好地掌握优化算法。
:::