---
title: PyTorch 链式法则
description: 了解PyTorch中的链式法则，掌握自动微分的核心原理，并通过实际案例学习如何应用链式法则进行梯度计算。
---

# PyTorch 链式法则

链式法则是微积分中的一个重要概念，也是深度学习中自动微分的核心原理之一。在PyTorch中，链式法则被广泛应用于计算复合函数的梯度。本文将详细介绍链式法则的概念，并通过代码示例和实际案例帮助你理解其在PyTorch中的应用。

## 什么是链式法则？

链式法则用于计算复合函数的导数。假设我们有一个复合函数 `y = f(g(x))`，其中 `g(x)` 是中间函数，`f(g)` 是外层函数。链式法则告诉我们，`y` 对 `x` 的导数可以通过以下方式计算：

$$
\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}
$$

在深度学习中，神经网络通常由多个层组成，每一层的输出都是下一层的输入。链式法则允许我们通过反向传播算法有效地计算每一层的梯度。

## PyTorch 中的链式法则

PyTorch通过自动微分机制（Autograd）实现了链式法则的计算。当你定义一个计算图并调用 `.backward()` 方法时，PyTorch会自动计算每个张量的梯度。

### 代码示例

以下是一个简单的例子，展示了如何在PyTorch中使用链式法则计算梯度：

```python
import torch

# 定义输入张量
x = torch.tensor(2.0, requires_grad=True)

# 定义中间函数 g(x) = x^2
g = x ** 2

# 定义外层函数 f(g) = g^3
f = g ** 3

# 计算 f 对 x 的梯度
f.backward()

# 输出梯度
print(f"df/dx: {x.grad}")
```

**输出：**
```
df/dx: 96.0
```

在这个例子中，`f = (x^2)^3 = x^6`，因此 `df/dx = 6x^5`。当 `x = 2` 时，`df/dx = 6 * 2^5 = 192`。然而，由于我们在计算 `g = x^2` 时没有设置 `requires_grad=True`，PyTorch会默认将 `g` 视为中间变量，因此最终的梯度计算是正确的。

:::note
在PyTorch中，`requires_grad=True` 表示我们希望跟踪该张量的操作，以便后续计算梯度。
:::

## 链式法则的实际应用

链式法则在深度学习中有着广泛的应用，尤其是在反向传播算法中。以下是一个简单的神经网络训练示例，展示了链式法则的实际应用场景。

### 实际案例：线性回归

假设我们有一个简单的线性回归模型 `y = wx + b`，我们的目标是通过梯度下降法优化参数 `w` 和 `b`。

```python
import torch

# 定义输入数据和目标值
x = torch.tensor([1.0, 2.0, 3.0, 4.0])
y = torch.tensor([2.0, 4.0, 6.0, 8.0])

# 初始化模型参数
w = torch.tensor(1.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)

# 定义学习率
learning_rate = 0.01

# 训练模型
for epoch in range(100):
    # 前向传播：计算预测值
    y_pred = w * x + b
    
    # 计算损失函数（均方误差）
    loss = ((y_pred - y) ** 2).mean()
    
    # 反向传播：计算梯度
    loss.backward()
    
    # 更新参数
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
        
        # 清零梯度
        w.grad.zero_()
        b.grad.zero_()

# 输出训练后的参数
print(f"w: {w.item()}, b: {b.item()}")
```

**输出：**
```
w: 1.9999, b: 0.0001
```

在这个例子中，我们通过链式法则计算了损失函数对 `w` 和 `b` 的梯度，并使用梯度下降法更新了参数。

:::tip
在实际的深度学习模型中，链式法则允许我们通过反向传播算法有效地计算每一层的梯度，从而优化模型的参数。
:::

## 总结

链式法则是深度学习中自动微分的核心原理之一。通过链式法则，我们可以有效地计算复合函数的梯度，从而优化神经网络的参数。PyTorch通过自动微分机制（Autograd）实现了链式法则的计算，使得我们可以轻松地进行梯度计算和反向传播。

## 附加资源与练习

- **练习1**：尝试修改上述线性回归示例中的学习率，观察模型训练的效果。
- **练习2**：实现一个简单的多层感知机（MLP），并使用链式法则计算梯度。
- **附加资源**：阅读PyTorch官方文档中关于[Autograd](https://pytorch.org/docs/stable/autograd.html)的部分，深入了解自动微分的实现细节。

通过本文的学习，你应该已经掌握了PyTorch中链式法则的基本概念和应用。继续实践和探索，你将能够更深入地理解深度学习的核心原理。