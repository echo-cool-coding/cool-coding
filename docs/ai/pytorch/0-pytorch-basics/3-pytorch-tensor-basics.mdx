---
title: PyTorch 张量基础
description: 本文介绍了PyTorch中的张量（Tensor）基础概念，包括张量的创建、操作以及实际应用场景，适合初学者快速入门。
---

# PyTorch 张量基础

## 介绍

在PyTorch中，**张量（Tensor）** 是最基本的数据结构，类似于NumPy中的数组（ndarray）。张量可以是一个标量、向量、矩阵或更高维度的数组。PyTorch的张量支持GPU加速计算，这使得它在深度学习中非常高效。

本文将逐步介绍如何创建和操作PyTorch张量，并通过实际案例展示其应用场景。

## 张量的创建

在PyTorch中，可以通过多种方式创建张量。以下是一些常见的创建方法：

### 1. 从Python列表创建张量

```python
import torch

# 创建一个1维张量（向量）
tensor_1d = torch.tensor([1.0, 2.0, 3.0])
print(tensor_1d)
```

**输出：**
```
tensor([1., 2., 3.])
```

### 2. 创建全零或全一张量

```python
# 创建一个2x3的全零矩阵
zeros_tensor = torch.zeros(2, 3)
print(zeros_tensor)

# 创建一个2x3的全一矩阵
ones_tensor = torch.ones(2, 3)
print(ones_tensor)
```

**输出：**
```
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
```

### 3. 创建随机张量

```python
# 创建一个2x3的随机矩阵
random_tensor = torch.rand(2, 3)
print(random_tensor)
```

**输出：**
```
tensor([[0.1234, 0.5678, 0.9101],
        [0.2345, 0.6789, 0.0123]])
```

:::tip
`torch.rand` 生成的随机数在区间 [0, 1) 上均匀分布。
:::

## 张量的操作

PyTorch提供了丰富的张量操作，以下是一些常见的操作示例。

### 1. 张量的形状和维度

```python
# 获取张量的形状
shape = random_tensor.shape
print(shape)

# 获取张量的维度
dim = random_tensor.dim()
print(dim)
```

**输出：**
```
torch.Size([2, 3])
2
```

### 2. 张量的索引和切片

```python
# 获取第一行的所有元素
row = random_tensor[0, :]
print(row)

# 获取第二列的所有元素
column = random_tensor[:, 1]
print(column)
```

**输出：**
```
tensor([0.1234, 0.5678, 0.9101])
tensor([0.5678, 0.6789])
```

### 3. 张量的数学运算

```python
# 张量加法
tensor_a = torch.tensor([1.0, 2.0, 3.0])
tensor_b = torch.tensor([4.0, 5.0, 6.0])
tensor_sum = tensor_a + tensor_b
print(tensor_sum)

# 张量乘法（逐元素相乘）
tensor_mul = tensor_a * tensor_b
print(tensor_mul)
```

**输出：**
```
tensor([5., 7., 9.])
tensor([ 4., 10., 18.])
```

:::note
PyTorch中的张量运算支持广播机制，类似于NumPy。
:::

## 实际应用场景

### 1. 线性回归中的张量应用

在机器学习中，线性回归模型通常使用张量来表示输入数据和模型参数。以下是一个简单的线性回归示例：

```python
# 输入数据
x = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[2.0], [4.0], [6.0]])

# 模型参数
w = torch.tensor([[1.0]], requires_grad=True)
b = torch.tensor([[0.0]], requires_grad=True)

# 前向传播
y_pred = x @ w + b

# 计算损失
loss = ((y_pred - y) ** 2).mean()
print(loss)
```

**输出：**
```
tensor(0., grad_fn=<MeanBackward0>)
```

:::caution
在实际训练中，通常需要多次迭代更新参数 `w` 和 `b` 以最小化损失。
:::

### 2. 图像处理中的张量应用

在图像处理中，图像通常被表示为3维张量（高度 x 宽度 x 通道）。以下是一个简单的图像处理示例：

```python
# 假设我们有一个3x3的RGB图像
image = torch.rand(3, 3, 3)  # 3x3像素，3个通道（RGB）

# 将图像转换为灰度图
gray_image = image.mean(dim=2)
print(gray_image)
```

**输出：**
```
tensor([[0.4567, 0.5678, 0.6789],
        [0.7890, 0.8901, 0.9012],
        [0.1234, 0.2345, 0.3456]])
```

## 总结

本文介绍了PyTorch中张量的基本概念、创建方法、常见操作以及实际应用场景。张量是PyTorch中最核心的数据结构，掌握张量的使用是学习PyTorch和深度学习的基础。

## 附加资源与练习

- **练习1**：创建一个3x3的随机张量，并计算其转置矩阵。
- **练习2**：使用张量实现一个简单的矩阵乘法。
- **附加资源**：PyTorch官方文档中的 [Tensors](https://pytorch.org/docs/stable/tensors.html) 部分。

通过不断练习和探索，你将能够更深入地理解PyTorch张量的强大功能！