---
title: PyTorch 线性层
description: 了解PyTorch中的线性层（Linear Layer），它是神经网络中最基础的层之一，用于实现全连接操作。本文将从概念、代码实现到实际应用场景，逐步讲解线性层的工作原理。
---

# PyTorch 线性层

在深度学习中，线性层（Linear Layer）是神经网络中最基础且常用的层之一。它也被称为全连接层（Fully Connected Layer），用于将输入数据通过线性变换映射到输出空间。本文将详细介绍PyTorch中的线性层，包括其工作原理、代码实现以及实际应用场景。

## 什么是线性层？

线性层是神经网络中的一种层结构，其核心功能是对输入数据进行线性变换。具体来说，线性层通过以下公式将输入数据映射到输出空间：

$$
\text{output} = \text{input} \times \text{weight}^T + \text{bias}
$$

其中：
- `input` 是输入数据，形状为 `(batch_size, in_features)`。
- `weight` 是权重矩阵，形状为 `(out_features, in_features)`。
- `bias` 是偏置向量，形状为 `(out_features)`。
- `output` 是输出数据，形状为 `(batch_size, out_features)`。

线性层的作用是将输入数据的每个特征与权重矩阵相乘，并加上偏置，从而得到输出数据。

## PyTorch 中的线性层

在PyTorch中，线性层由 `torch.nn.Linear` 类实现。以下是一个简单的示例，展示如何使用线性层：

```python
import torch
import torch.nn as nn

# 定义一个线性层，输入特征数为5，输出特征数为3
linear_layer = nn.Linear(in_features=5, out_features=3)

# 创建一个随机输入张量，形状为 (batch_size, in_features)
input_tensor = torch.randn(2, 5)

# 将输入张量传递给线性层
output_tensor = linear_layer(input_tensor)

print("输入张量:\n", input_tensor)
print("输出张量:\n", output_tensor)
```

**输出示例：**
```
输入张量:
 tensor([[ 0.1234, -0.5678,  0.9101, -0.2345,  0.6789],
         [-0.3456,  0.7890, -0.1234,  0.4567, -0.8901]])
输出张量:
 tensor([[ 0.2345, -0.6789,  0.1234],
         [-0.4567,  0.8901, -0.3456]], grad_fn=<AddmmBackward>)
```

在这个示例中，我们定义了一个输入特征数为5、输出特征数为3的线性层。输入张量的形状为 `(2, 5)`，表示有2个样本，每个样本有5个特征。经过线性层后，输出张量的形状变为 `(2, 3)`，表示每个样本被映射到了3个输出特征。

:::note
`torch.nn.Linear` 的权重和偏置是随机初始化的，但可以通过 `linear_layer.weight` 和 `linear_layer.bias` 访问和修改。
:::

## 线性层的工作原理

为了更好地理解线性层的工作原理，我们可以将其分解为以下几个步骤：

1. **输入数据**：输入数据的形状为 `(batch_size, in_features)`，其中 `batch_size` 是批次大小，`in_features` 是输入特征数。
2. **权重矩阵**：权重矩阵的形状为 `(out_features, in_features)`，其中 `out_features` 是输出特征数。
3. **偏置向量**：偏置向量的形状为 `(out_features)`，每个输出特征对应一个偏置值。
4. **线性变换**：输入数据与权重矩阵相乘，并加上偏置向量，得到输出数据。

以下是一个简单的数学示例：

假设输入数据为：
$$
\text{input} = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}
$$

权重矩阵为：
$$
\text{weight} = \begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \end{bmatrix}
$$

偏置向量为：
$$
\text{bias} = \begin{bmatrix} 0.1 & 0.2 \end{bmatrix}
$$

则输出数据为：
$$
\text{output} = \text{input} \times \text{weight}^T + \text{bias} = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \times \begin{bmatrix} 0.1 & 0.4 \\ 0.2 & 0.5 \\ 0.3 & 0.6 \end{bmatrix} + \begin{bmatrix} 0.1 & 0.2 \end{bmatrix} = \begin{bmatrix} 1.4 & 3.2 \end{bmatrix}
$$

## 实际应用场景

线性层在神经网络中的应用非常广泛。以下是一些常见的应用场景：

1. **全连接神经网络**：线性层是构建全连接神经网络的基础。通过堆叠多个线性层和非线性激活函数，可以构建复杂的神经网络模型。
2. **特征提取**：线性层可以用于将高维输入数据映射到低维空间，从而实现特征提取。
3. **回归问题**：在回归问题中，线性层可以用于预测连续值。例如，预测房价、股票价格等。

以下是一个简单的回归问题示例，使用线性层预测房价：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义数据集
X = torch.tensor([[1.0], [2.0], [3.0], [4.0]], requires_grad=True)
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]], requires_grad=True)

# 定义模型
model = nn.Linear(in_features=1, out_features=1)

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 训练模型
for epoch in range(100):
    # 前向传播
    y_pred = model(X)
    
    # 计算损失
    loss = criterion(y_pred, y)
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# 测试模型
test_input = torch.tensor([[5.0]])
predicted_output = model(test_input)
print("预测输出:", predicted_output.item())
```

**输出示例：**
```
预测输出: 10.0
```

在这个示例中，我们使用线性层构建了一个简单的回归模型，用于预测房价。通过训练模型，我们可以得到接近真实值的预测结果。

## 总结

线性层是神经网络中最基础的层之一，用于实现全连接操作。通过本文的介绍，我们了解了线性层的工作原理、代码实现以及实际应用场景。希望本文能帮助你更好地理解和使用PyTorch中的线性层。

## 附加资源与练习

- **练习1**：尝试修改线性层的输入特征数和输出特征数，观察输出张量的形状变化。
- **练习2**：使用线性层构建一个简单的分类模型，并在MNIST数据集上进行训练。
- **附加资源**：
  - [PyTorch官方文档 - torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)
  - 《深度学习入门：基于Python的理论与实现》——斋藤康毅

:::tip
如果你对线性层的工作原理还有疑问，建议通过调试代码或绘制计算图来加深理解。
:::