---
title: PyTorch 激活函数
description: 了解PyTorch中的激活函数，包括其作用、常见类型以及如何在神经网络中使用它们。
---

# PyTorch 激活函数

激活函数是神经网络中至关重要的组成部分。它们为模型引入了非线性特性，使得神经网络能够学习和表示复杂的模式。在本教程中，我们将深入探讨PyTorch中的激活函数，包括它们的定义、常见类型以及如何在神经网络中使用它们。

## 什么是激活函数？

激活函数是神经网络中的一个数学函数，它决定了神经元的输出。简单来说，激活函数将输入的加权和转换为输出信号，传递给下一层神经元。如果没有激活函数，神经网络将只是一个线性回归模型，无法处理复杂的任务。

### 为什么需要激活函数？

激活函数的主要作用是引入非线性。如果没有激活函数，无论神经网络有多少层，它都只能表示线性关系。通过使用激活函数，神经网络可以学习和表示复杂的非线性关系。

## 常见的激活函数

PyTorch提供了多种激活函数，以下是一些最常见的激活函数及其用途：

### 1. ReLU（Rectified Linear Unit）

ReLU是最常用的激活函数之一。它的定义非常简单：

```
f(x) = max(0, x)
```

ReLU将所有负值置为0，而正值保持不变。它的计算效率高，且在大多数情况下表现良好。

```python
import torch
import torch.nn as nn

# 创建一个ReLU激活函数实例
relu = nn.ReLU()

# 输入张量
input_tensor = torch.tensor([-1.0, 2.0, -3.0, 4.0])

# 应用ReLU激活函数
output_tensor = relu(input_tensor)
print(output_tensor)  # 输出: tensor([0., 2., 0., 4.])
```

### 2. Sigmoid

Sigmoid函数将输入值压缩到0和1之间。它的定义如下：

```
f(x) = 1 / (1 + exp(-x))
```

Sigmoid函数常用于二分类问题的输出层。

```python
# 创建一个Sigmoid激活函数实例
sigmoid = nn.Sigmoid()

# 输入张量
input_tensor = torch.tensor([-1.0, 2.0, -3.0, 4.0])

# 应用Sigmoid激活函数
output_tensor = sigmoid(input_tensor)
print(output_tensor)  # 输出: tensor([0.2689, 0.8808, 0.0474, 0.9820])
```

### 3. Tanh

Tanh函数将输入值压缩到-1和1之间。它的定义如下：

```
f(x) = tanh(x)
```

Tanh函数在隐藏层中常用，因为它将输入值标准化到-1和1之间。

```python
# 创建一个Tanh激活函数实例
tanh = nn.Tanh()

# 输入张量
input_tensor = torch.tensor([-1.0, 2.0, -3.0, 4.0])

# 应用Tanh激活函数
output_tensor = tanh(input_tensor)
print(output_tensor)  # 输出: tensor([-0.7616,  0.9640, -0.9951,  0.9993])
```

### 4. Softmax

Softmax函数通常用于多分类问题的输出层。它将输入值转换为概率分布，使得所有输出的和为1。

```python
# 创建一个Softmax激活函数实例
softmax = nn.Softmax(dim=1)

# 输入张量
input_tensor = torch.tensor([[1.0, 2.0, 3.0]])

# 应用Softmax激活函数
output_tensor = softmax(input_tensor)
print(output_tensor)  # 输出: tensor([[0.0900, 0.2447, 0.6652]])
```

## 实际应用案例

假设我们正在构建一个简单的神经网络来对手写数字进行分类。我们可以使用ReLU作为隐藏层的激活函数，并使用Softmax作为输出层的激活函数。

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义一个简单的神经网络
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# 创建模型实例
model = SimpleNN()

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 假设我们有一些输入数据
input_data = torch.randn(1, 784)  # 1个样本，784个特征
target = torch.tensor([5])  # 目标类别

# 前向传播
output = model(input_data)

# 计算损失
loss = criterion(output, target)

# 反向传播和优化
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

## 总结

激活函数是神经网络中不可或缺的一部分，它们为模型引入了非线性特性，使得神经网络能够学习和表示复杂的模式。在本教程中，我们介绍了PyTorch中的几种常见激活函数，包括ReLU、Sigmoid、Tanh和Softmax，并通过实际案例展示了如何在神经网络中使用它们。

## 附加资源

- [PyTorch官方文档](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
- [深度学习中的激活函数](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)

## 练习

1. 尝试在PyTorch中实现一个简单的神经网络，并使用不同的激活函数观察其效果。
2. 修改上面的代码，使用Sigmoid作为隐藏层的激活函数，并比较其与ReLU的性能差异。
