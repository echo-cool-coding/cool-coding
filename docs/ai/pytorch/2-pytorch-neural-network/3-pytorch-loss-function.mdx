---
title: PyTorch 损失函数
description: 了解PyTorch中的损失函数，掌握如何衡量模型预测与真实值之间的差异，并通过实际案例学习其应用。
---

# PyTorch 损失函数

在深度学习中，损失函数（Loss Function）是衡量模型预测值与真实值之间差异的关键工具。通过最小化损失函数，模型可以逐步优化其参数，从而提高预测的准确性。PyTorch提供了多种内置的损失函数，适用于不同的任务，如回归、分类等。本文将详细介绍PyTorch中的损失函数，并通过代码示例和实际案例帮助你理解其应用。

## 什么是损失函数？

损失函数是模型训练过程中用于衡量预测值与真实值之间差异的函数。它的值越小，表示模型的预测越接近真实值。损失函数的选择取决于具体的任务类型。例如，在回归任务中，常用的损失函数是均方误差（MSE），而在分类任务中，交叉熵损失（Cross-Entropy Loss）更为常见。

## 常见的PyTorch损失函数

PyTorch提供了多种内置的损失函数，以下是一些常见的损失函数及其应用场景：

### 1. 均方误差损失（MSELoss）

均方误差损失（Mean Squared Error Loss）是回归任务中最常用的损失函数之一。它计算预测值与真实值之间的平方差的平均值。

```python
import torch
import torch.nn as nn

# 创建预测值和真实值
predicted = torch.tensor([0.5, 0.2, 0.9], requires_grad=True)
target = torch.tensor([1.0, 0.0, 1.0])

# 定义损失函数
criterion = nn.MSELoss()

# 计算损失
loss = criterion(predicted, target)
print(f"MSELoss: {loss.item()}")
```

**输出：**
```
MSELoss: 0.11666666716337204
```

### 2. 交叉熵损失（CrossEntropyLoss）

交叉熵损失（Cross-Entropy Loss）通常用于分类任务，特别是多分类问题。它结合了 `LogSoftmax` 和 `NLLLoss`，直接接受模型的原始输出（未经过 `Softmax` 处理）。

```python
# 创建预测值和真实标签
predicted = torch.tensor([[2.0, 1.0, 0.1], [1.0, 2.0, 0.1]])
target = torch.tensor([0, 1])  # 类别标签

# 定义损失函数
criterion = nn.CrossEntropyLoss()

# 计算损失
loss = criterion(predicted, target)
print(f"CrossEntropyLoss: {loss.item()}")
```

**输出：**
```
CrossEntropyLoss: 0.4170299470424652
```

### 3. 二元交叉熵损失（BCELoss）

二元交叉熵损失（Binary Cross-Entropy Loss）用于二分类任务。它要求预测值经过 `Sigmoid` 处理，输出在 0 到 1 之间。

```python
# 创建预测值和真实标签
predicted = torch.tensor([0.7, 0.2, 0.9], requires_grad=True)
target = torch.tensor([1.0, 0.0, 1.0])

# 定义损失函数
criterion = nn.BCELoss()

# 计算损失
loss = criterion(predicted, target)
print(f"BCELoss: {loss.item()}")
```

**输出：**
```
BCELoss: 0.23103654384613037
```

## 实际案例：使用损失函数训练模型

让我们通过一个简单的线性回归模型来展示如何使用损失函数进行模型训练。

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 创建数据集
x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])

# 定义线性模型
model = nn.Linear(1, 1)

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 训练模型
for epoch in range(100):
    # 前向传播
    outputs = model(x)
    loss = criterion(outputs, y)
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')

# 测试模型
predicted = model(torch.tensor([[5.0]]))
print(f'Predicted value for input 5.0: {predicted.item()}')
```

**输出：**
```
Epoch [10/100], Loss: 0.0012
Epoch [20/100], Loss: 0.0007
...
Epoch [100/100], Loss: 0.0001
Predicted value for input 5.0: 9.9998
```

:::tip
在实际训练中，选择合适的损失函数和优化器对模型的性能至关重要。不同的任务需要不同的损失函数，因此在设计模型时务必根据任务类型选择合适的损失函数。
:::

## 总结

损失函数是深度学习中不可或缺的一部分，它帮助我们衡量模型的预测效果，并通过优化算法调整模型参数。本文介绍了PyTorch中常见的损失函数，并通过代码示例展示了如何使用这些损失函数进行模型训练。希望本文能帮助你更好地理解损失函数的概念及其在深度学习中的应用。

## 附加资源与练习

- **练习1**：尝试使用 `CrossEntropyLoss` 训练一个简单的分类模型，数据集可以使用 `torchvision.datasets.MNIST`。
- **练习2**：研究 `nn.BCEWithLogitsLoss` 与 `nn.BCELoss` 的区别，并尝试在二分类任务中使用它们。

:::note
如果你对损失函数的使用还有疑问，可以参考 [PyTorch官方文档](https://pytorch.org/docs/stable/nn.html#loss-functions) 获取更多信息。
:::