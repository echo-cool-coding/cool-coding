---
title: PyTorch ONNX 转换
description: 学习如何将 PyTorch 模型转换为 ONNX 格式，以便在不同框架和硬件上运行模型推理。
---

# PyTorch ONNX 转换

在深度学习中，模型的训练和推理通常在不同的环境中进行。PyTorch 是一个流行的深度学习框架，但有时我们需要将模型导出为其他格式，以便在其他框架或硬件上运行。ONNX（Open Neural Network Exchange）是一种开放的模型格式，支持跨框架和跨平台的模型转换。本文将介绍如何将 PyTorch 模型转换为 ONNX 格式。

## 什么是 ONNX？

ONNX 是一种开放的模型格式，旨在使深度学习模型能够在不同的框架和硬件之间无缝转换和运行。通过将模型转换为 ONNX 格式，您可以在支持 ONNX 的框架（如 TensorFlow、Caffe2、MXNet 等）中运行模型，或者在支持 ONNX 的硬件加速器上进行推理。

## 为什么需要 ONNX 转换？

1. **跨框架兼容性**：ONNX 允许您将模型从一个框架（如 PyTorch）导出，并在另一个框架（如 TensorFlow）中运行。
2. **硬件加速**：某些硬件加速器（如 NVIDIA TensorRT）支持 ONNX 格式，可以显著提高推理速度。
3. **模型部署**：ONNX 格式的模型可以更容易地部署到生产环境中，尤其是在需要跨平台支持的情况下。

## 如何将 PyTorch 模型转换为 ONNX 格式

### 1. 安装 ONNX 和 ONNX Runtime

首先，您需要安装 ONNX 和 ONNX Runtime。可以使用以下命令进行安装：

```bash
pip install onnx onnxruntime
```

### 2. 导出 PyTorch 模型为 ONNX 格式

假设我们有一个简单的 PyTorch 模型，如下所示：

```python
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x):
        return self.fc(x)

model = SimpleModel()
```

要将这个模型导出为 ONNX 格式，可以使用 `torch.onnx.export` 函数：

```python
import torch

# 创建一个示例输入
dummy_input = torch.randn(1, 10)

# 导出模型为 ONNX 格式
torch.onnx.export(model,               # 要导出的模型
                  dummy_input,         # 示例输入
                  "simple_model.onnx", # 导出的文件名
                  export_params=True,  # 导出模型参数
                  opset_version=11,    # ONNX 操作集版本
                  do_constant_folding=True,  # 是否进行常量折叠优化
                  input_names = ['input'],   # 输入名称
                  output_names = ['output'], # 输出名称
                  dynamic_axes={'input' : {0 : 'batch_size'},    # 动态轴
                                'output' : {0 : 'batch_size'}})
```

### 3. 验证 ONNX 模型

导出 ONNX 模型后，您可以使用 ONNX Runtime 来验证模型是否正确导出并运行：

```python
import onnx
import onnxruntime as ort

# 加载 ONNX 模型
onnx_model = onnx.load("simple_model.onnx")
onnx.checker.check_model(onnx_model)

# 使用 ONNX Runtime 运行模型
ort_session = ort.InferenceSession("simple_model.onnx")

# 准备输入数据
input_data = dummy_input.numpy()

# 运行推理
outputs = ort_session.run(None, {'input': input_data})

print(outputs)
```

### 4. 动态轴支持

在上面的代码中，我们使用了 `dynamic_axes` 参数来指定输入和输出的动态轴。这意味着模型可以处理不同大小的输入批次。例如，如果您有一个批次大小为 1 的输入，模型也可以处理批次大小为 10 的输入。

## 实际应用场景

### 1. 跨框架部署

假设您在一个项目中使用 PyTorch 进行模型训练，但需要在 TensorFlow 中进行推理。通过将模型导出为 ONNX 格式，您可以轻松地在 TensorFlow 中加载和运行模型。

### 2. 硬件加速

某些硬件加速器（如 NVIDIA TensorRT）支持 ONNX 格式。通过将模型转换为 ONNX 格式，您可以在这些硬件上运行模型，从而显著提高推理速度。

### 3. 模型优化

ONNX 提供了一些工具和库（如 ONNX Runtime）来优化模型推理。通过将模型转换为 ONNX 格式，您可以利用这些工具来优化模型的推理性能。

## 总结

通过将 PyTorch 模型转换为 ONNX 格式，您可以实现跨框架和跨平台的模型部署和推理。本文介绍了如何使用 `torch.onnx.export` 函数将 PyTorch 模型导出为 ONNX 格式，并展示了如何验证和运行 ONNX 模型。我们还讨论了 ONNX 转换的实际应用场景，包括跨框架部署、硬件加速和模型优化。

## 附加资源

- [ONNX 官方文档](https://onnx.ai/)
- [PyTorch ONNX 导出文档](https://pytorch.org/docs/stable/onnx.html)
- [ONNX Runtime 官方文档](https://onnxruntime.ai/)

## 练习

1. 尝试将一个更复杂的 PyTorch 模型（如卷积神经网络）导出为 ONNX 格式。
2. 使用 ONNX Runtime 在不同的硬件（如 CPU 和 GPU）上运行导出的模型，并比较推理速度。
3. 探索 ONNX 提供的其他工具和库，如 ONNX Optimizer，并尝试优化导出的模型。

通过完成这些练习，您将更深入地理解 PyTorch ONNX 转换的过程，并能够在实际项目中应用这些知识。